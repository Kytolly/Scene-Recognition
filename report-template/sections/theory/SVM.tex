\subsection{支持向量机 (SVM) 实验原理}

支持向量机 (Support Vector Machine, SVM) 是一种强大的监督学习模型，
主要用于分类和回归任务。在本次场景分类实验中，
我们将使用 SVM 作为 Bag of Words 特征的分类器。

SVM 的核心思想是寻找一个最优的超平面 (Hyperplane) 来在高维特征空间中将不同类别的样本分开。
这个最优超平面不仅要能正确划分样本，
还要使两类样本中的支持向量到超平面的间隔（Margin）最大化。
最大化间隔可以提高分类器的泛化能力，使其在新数据上表现更好。

\subsubsection{数学原理}

对于一个二分类问题，给定一组带有标签的训练数据，SVM 的目标是找到一个决策函数，通常是线性的：
    \[ f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b \]
其中 \(\mathbf{w}\) 是超平面的法向量，\(b\) 是偏置项。超平面由 \(\mathbf{w} \cdot \mathbf{x} + b = 0\) 定义。

对于训练样本 \((x_i, y_i)\)，
其中 \(x_i\) 是特征向量，
\(y_i \in \{-1, 1\}\) 是类别标签，
我们希望找到 \(\mathbf{w}\) 和 \(b\)，使得：
    \[ y_i (\mathbf{w} \cdot x_i + b) \ge 1 \]
同时，我们希望最小化 \(\|\mathbf{w}\|^2\)，
等价于最大化间隔 \(2/\|\mathbf{w}\|\)），
这是一个凸优化问题，可以通过拉格朗日乘子法等技术求解。

在实际应用中，数据往往不是完全线性可分的。
SVM 引入了\textbf{软间隔 (Soft Margin)} 的概念，
允许少量样本点违反间隔约束，
即允许一些样本点位于间隔带内甚至错误的一侧。
这通过引入松弛变量 \(\xi_i \ge 0\) 和惩罚参数 \(C\) 来实现，优化目标变为最小化：
    \[ \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \]
约束条件变为：
\[ y_i (\mathbf{w} \cdot x_i + b) \ge 1 - \xi_i \]

\subsubsection{Kernel Trick}

对于非线性可分的数据，SVM 使用\textbf{核技巧 (Kernel Trick)} 
将原始特征空间映射到更高维的空间，
使得样本在该高维空间中变得线性可分。
常用的核函数包括多项式核、径向基函数 (RBF) 核等。核函数 \(K(\mathbf{x}_i, \mathbf{x}_j)\) 计算的是样本在映射后的高维空间中的内积，而无需显式计算高维映射本身，这大大提高了计算效率。

然而，在本次实验中，由于使用了 `sklearn.svm.LinearSVC`，
这表示我们主要考虑线性分类器，
或者等价于使用了线性核 
    \(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j\)。
线性 SVM 在处理高维稀疏数据时通常非常高效，这与 Bag of Words 特征的特性相符。

\subsubsection{多类别分类}

SVM 本身是二分类器。对于像场景分类这样的多类别任务（假设有 M 个类别），
需要将二分类 SVM 扩展。常用的策略有两种：

\begin{enumerate}
    \item \textbf{一对多 (One-vs-Rest, OvR):} 为每个类别训练一个二分类 SVM。例如，对于类别 k，训练一个分类器来区分类别 k 的样本和所有非类别 k 的样本。总共需要训练 M 个分类器。在预测时，将待分类样本输入所有 M 个分类器，选择输出分数最高（或离超平面最远）的那个类别作为预测结果。`LinearSVC` 默认通常采用 One-vs-Rest 策略。
    \item \textbf{一对一 (One-vs-One, OvO):} 为每一对不同的类别训练一个二分类 SVM。例如，对于类别 i 和类别 j，训练一个分类器来区分这两类样本。总共需要训练 \(M(M-1)/2\) 个分类器。在预测时，将待分类样本输入所有分类器，然后使用投票机制决定最终类别。每个分类器都为其中一个类别投一票，得票最多的类别获胜。
\end{enumerate}
在本次实验中，我们使用的 `LinearSVC` 通常采用 One-vs-Rest 策略来实现多类别分类。