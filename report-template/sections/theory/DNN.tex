\subsection{DNN原理}

在传统的图像分类方法中，特征提取（如 Tiny Images, SIFT+BoW）和分类是分开进行的两个步骤。而深度神经网络 (DNN)，尤其是卷积神经网络 (CNN)，能够将特征学习和分类集成到一个端到端的模型中，通过多层非线性变换自动从原始像素数据中学习到具有判别力的层次化特征。

\subsubsection{深度神经网络 (DNN) 原理}

深度神经网络由多个层组成，每一层都对输入进行某种变换，
并将结果传递给下一层。通过堆叠多层，
网络能够学习越来越抽象和复杂的特征表示。
对于图像任务，\textbf{卷积神经网络 (CNN)} 是最成功的 DNN 架构。
CNN 的核心在于其能够有效地处理具有网格状拓扑结构的数据（如图像）。

一个典型的 CNN 架构通常包含以下几种类型的层：

\begin{itemize}
    \item \textbf{卷积层 (Convolutional Layer):} 通过一组可学习的滤波器对输入图像进行卷积操作，提取局部特征。每个滤波器学习一种特定的空间模式。卷积操作通过权值共享和利用图像的空间局部相关性减少参数数量。
    \item \textbf{激活函数层 (Activation Layer):} 在卷积操作之后，应用非线性激活函数（如 ReLU）。非线性是学习复杂模式的关键。
    \item \textbf{池化层 (Pooling Layer):} 用于减小特征图的空间尺寸，同时保留重要信息。常见的有最大池化和平均池化，提供平移不变性。
    \item \textbf{全连接层 (Fully Connected Layer):} 将高级特征映射到最终的输出（如类别得分）。每个神经元与前一层的所有神经元相连。
    \item \textbf{输出层 (Output Layer):} 最后一层，通常结合 Softmax 转换为类别概率分布。
\end{itemize}

\subsubsection{PyTorch框架部署DNN}

PyTorch 提供了构建、训练和部署 DNN 模型的灵活工具。
在 PyTorch 中实现基于 CNN 的场景分类任务，通常遵循以下流程：
通过 PyTorch，我们可以灵活地定义复杂的 CNN 结构，
利用其自动求导机制进行高效训练，并方便地在 GPU 上加速计算。
